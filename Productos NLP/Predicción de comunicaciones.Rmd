---
title: "Predicción de comunicaciones"
author: "Daniel Jiménez"
date: "12/14/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```




### Predicción de lo que se comenta en el texto


```{r}
library(keras)
library(tidyverse)
library(tokenizers)
library(tidyverse)
library(tidymodels)
library(tidytext)
library(widyr)
library(textrank)
library(stopwords)
library(scales)
library(igraph) 

theme_set(theme_classic())
```



```{r}

colombia <- read_rds('/Volumes/Daniel/BID/Bases_rds_BID/colombia.rds')

comunicado_0016 <- colombia%>%
  filter(Comunicado == '0016')%>%
  pull(Texto)

article_sentences <- tibble(text = comunicado_0016) %>%
  unnest_tokens(sentence, text, token = "sentences") %>%
  mutate(sentence_id = row_number()) %>%
  select(sentence_id, sentence)
```

```{r}
text <- article_sentences %>% 
  mutate(sentence = str_replace_all(sentence, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)", "")) %>%
  mutate(sentence = tolower(sentence),
         sentence = str_replace_all(sentence, "[^a-z.\\s]", ""),
         sentence = str_replace_all(sentence, "\n|\r", "")) %>% 
  pull(sentence) %>% 
  str_to_lower() %>% 
  str_c(collapse = "\n") %>% 
  tokenize_characters(strip_non_alphanum = FALSE, simplify = TRUE)

chars <- text %>%
  unique() %>%
  sort()
maxlen <- 15

dataset <- map(
  seq(1, length(text) - maxlen - 1, by = 3),
  ~list(sentece = text[.x:(.x + maxlen - 1)], next_char = text[.x + maxlen])
)
dataset <- transpose(dataset)

x <- array(0, dim = c(length(dataset$sentece), maxlen, length(chars)))
y <- array(0, dim = c(length(dataset$sentece), length(chars)))
for(i in 1:length(dataset$sentece)){
  
  x[i,,] <- sapply(chars, function(x){
    as.integer(x == dataset$sentece[[i]])
  })
  
  y[i,] <- as.integer(chars == dataset$next_char[[i]])
  
}

model <- keras_model_sequential()
model %>%
  layer_lstm(128, input_shape = c(maxlen, length(chars))) %>%
  layer_dense(length(chars)) %>%
  layer_activation("softmax")
optimizer <- optimizer_rmsprop(lr = 0.01)
model %>% compile(
  loss = "categorical_crossentropy",
  optimizer = optimizer
)

sample_mod <- function(preds, temperature = 1){
  preds <- log(preds)/temperature
  exp_preds <- exp(preds)
  preds <- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %>%
    as.integer() %>%
    which.max()
}
on_epoch_end <- function(epoch, logs) {
  
  cat(sprintf("epoch: %02d ---------------\n\n", epoch))
  
  for(diversity in c(0.01,0.05,0.2, 0.5, 1, 1.2)){
    
    cat(sprintf("diversity: %f ---------------\n\n", diversity))
    
    start_index <- sample(1:(length(text) - maxlen), size = 1)
    sentence <- text[start_index:(start_index + maxlen - 1)]
    generated <- ""
    
    for(i in 1:400){
      
      x <- sapply(chars, function(x){
        as.integer(x == sentence)
      })
      x <- array_reshape(x, c(1, dim(x)))
      
      preds <- predict(model, x)
      next_index <- sample_mod(preds, diversity)
      next_char <- chars[next_index]
      
      generated <- str_c(generated, next_char, collapse = "")
      sentence <- c(sentence[-1], next_char)
      
    }
    
    cat(generated)
    cat("\n\n")
    
  }
}

sample_mod <- function(preds, temperature = 1){
  preds <- log(preds)/temperature
  exp_preds <- exp(preds)
  preds <- exp_preds/sum(exp(preds))
  
  rmultinom(1, 1, preds) %>%
    as.integer() %>%
    which.max()
}

print_callback <- callback_lambda(on_epoch_end = on_epoch_end)
model %>% fit(
  x, y,
  batch_size = 128,
  epochs = 20,
  callbacks = print_callback
)

generate_phrase <- function(model, text, chars, max_length, diversity){
  choose_next_char <- function(preds, chars, temperature){
    preds <- log(preds) / temperature
    exp_preds <- exp(preds)
    preds <- exp_preds / sum(exp(preds))
    
    next_index <- rmultinom(1, 1, preds) %>% 
      as.integer() %>%
      which.max()
    chars[next_index]
  }
  
  convert_sentence_to_data <- function(sentence, chars){
    x <- sapply(chars, function(x){
      as.integer(x == sentence)
    })
    array_reshape(x, c(1, dim(x)))
  }
  
  start_index <- sample(1:(length(text) - max_length), size = 1)
  sentence <- text[start_index:(start_index + max_length - 1)]
  generated <- ""
  
  for(i in 1:(max_length * 20)){
    sentence_data <- convert_sentence_to_data(sentence, chars)
    preds <- predict(model, sentence_data)
    next_char <- choose_next_char(preds, chars, diversity)
    generated <- str_c(generated, next_char, collapse = "")
    sentence <- c(sentence[-1], next_char)
  }
  
  generated
}


generate_phrase(model, text, chars, maxlen, diversity = .5)
```

